"""
Gemini API Client Wrapper - Compatible with generateContent endpoint
"""

import logging
import requests
from app.config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()

class GeminiService:
    BASE_URL = f"https://generativelanguage.googleapis.com/v1beta/models/{settings.GEMINI_MODEL}:generateContent"

    @staticmethod
    def generate_question(prompt: str, temperature: float = None, max_tokens: int = None) -> str:
        """
        Call Gemini API with prompt to generate question text.
        Uses the correct request structure for :generateContent endpoint.
        Includes debug logs for requests and responses.
        """
        temperature = temperature if temperature is not None else settings.GEMINI_TEMPERATURE
        max_tokens = max_tokens if max_tokens is not None else settings.GEMINI_MAX_TOKENS

        headers = {
            "Content-Type": "application/json",
            "X-Goog-Api-Key": settings.GEMINI_API_KEY
        }
        body = {
            "contents": [
                {
                    "parts": [
                        {"text": prompt}
                    ]
                }
            ],
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens
            }
        }
        logger.debug(f"Gemini request prompt: {prompt}")

        try:
            response = requests.post(GeminiService.BASE_URL, headers=headers, json=body)
            logger.debug(f"Gemini raw response: {response.text}")
            response.raise_for_status()
            data = response.json()
            # Adapt extraction depending on Gemini version used
            # Gemini responses typically: data['candidates'][0]['content']['parts'][0]['text']
            candidates = data.get("candidates", [])
            if candidates and "content" in candidates[0]:
                question_text = candidates[0]["content"]["parts"][0]["text"].strip()
            elif candidates and "output" in candidates[0]:  # for backward compatibility
                question_text = candidates[0]["output"].strip()
            else:
                logger.error(f"No text found in Gemini API response: {data}")
                raise ValueError("No question generated by Gemini API")
            logger.info(f"Gemini generated question: {question_text}")
            return question_text
        except requests.HTTPError as e:
            logger.error(f"HTTP error during Gemini call: {e} - Response: {response.text}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error during Gemini call: {e}")
            raise

    @staticmethod
    def generate_hr_question(job_role: str, industry: str) -> str:
        prompt = (
            f"Generate a concise, focused HR interview question for the job role '{job_role}' "
            f"in the '{industry}' industry. "
            "The question should be open-ended and assess the candidate's soft skills. "
            "Do NOT include explanations or extra commentary."
        )
        return GeminiService.generate_question(prompt)

    @staticmethod
    def generate_technical_question(job_role: str, skills: str) -> str:
        prompt = (
            f"Generate a concise, clear technical interview question for the job role '{job_role}' "
            f"with skills including {skills}. "
            "The question should test problem-solving or coding skills. "
            "Do NOT include explanations or additional text."
        )
        return GeminiService.generate_question(prompt)

    @staticmethod
    def generate_experience_question(user_profile: str) -> str:
        prompt = (
            f"Based on the following user profile, generate a personalized experience question: {user_profile}"
        )
        return GeminiService.generate_question(prompt)

    # ðŸ”¥ ADD THIS METHOD to GeminiService class

    @staticmethod
    def generate_question_for_type(question_type: str, user) -> str:
        """Generate based on type + user profile"""
        job_role = getattr(user, 'job_role', 'Software Engineer')
        skills_str = ' '.join(user.skills) if hasattr(user, 'skills') and user.skills else ''

        if question_type == "hr":
            return GeminiService.generate_hr_question(job_role, user.industry)
        elif question_type == "technical":
            return GeminiService.generate_technical_question(job_role, skills_str)
        elif question_type == "experience":
            profile = f"Profile: {user.bio or ''}. Skills: {skills_str}"
            return GeminiService.generate_experience_question(profile)
        else:
            return GeminiService.generate_question(f"Generate {question_type} question for {user.industry}")

